defmodule LazyDoc.Providers.GithubAi do
  @moduledoc """

   ## Main functionality

   The module LazyDoc.Providers.GithubAi provides a way of interacting with the Github AI API for prompt-based communication and response generation.

   ## Description

   It implements the behavior Provider, offering a standardized method to request and retrieve responses from AI models hosted on the Github AI platform. Key operations include sending prompts, constructing API requests, and processing responses.
  """
  @behaviour LazyDoc.Provider

  @github_ai_endpoint "https://models.github.ai/inference/chat/completions"

  ## TO_DO: make timeout,temperature, top_p and max_tokens customizable but with default values.
  ## TO_DO: implement retry with customizable number of retries if fails but with default value.
  @spec request_prompt(binary(), binary(), binary()) ::
          {:ok, Req.Response.t()} | {:error, Exception.t()}
  @doc """

  Parameters

  prompt - the input message to be processed by the model.
  model - the identifier for the specific AI model to be used.
  token - the authorization token for accessing the AI service.

  Description
   Sends a prompt to an AI model and returns the generated response.

  Returns
   the response generated by the AI model based on the provided prompt.
  """
  def request_prompt(prompt, model, token) do
    req_query(prompt, model, token)
    |> Req.post()
  end

  @doc """

  Parameters

  prompt - the input text or question to be sent to the model.
  model - the identifier for the specific model to use for generating a response.
  token - the authorization token required for accessing the API.

  Description
   Constructs a request to query an AI model with a given prompt and specified parameters.

  Returns
   a structured request object ready to interact with the API at the configured endpoint.

  """
  def req_query(prompt, model, token) do
    body = %{
      max_tokens: 2048,
      messages: [%{"role" => "system", "content" => ""}, %{"role" => "user", "content" => prompt}],
      model: "#{model}",
      temperature: 1,
      top_p: 1
    }

    Req.new(base_url: @github_ai_endpoint, json: body, receive_timeout: 240_000)
    |> Req.Request.put_header("Accept", "application/json")
    |> Req.Request.put_header("Content-Type", "application/json;charset=UTF-8")
    |> Req.Request.put_header("Authorization", "Bearer #{token}")
    |> Req.Steps.encode_body()
  end

  ## TO_DO: we should review if for each model in Github you have the same response format in the body.
  ## Maybe this premise is not true and it will require changes.
  @spec get_docs_from_response(Req.Response.t()) :: binary()
  @doc """

  Parameters

  response - a %Req.Response struct containing the response from an HTTP request.
  Description
   Parses the body of the HTTP response and extracts the message content from it.

  Returns
   the content of the message extracted from the response body.

  """
  def get_docs_from_response(%Req.Response{body: body} = _response) do
    ## Take always first choice
    message = Enum.at(body["choices"], 0)["message"]["content"]
    message
  end

  @spec model(atom()) :: binary()
  @doc """

  Parameters

  model - a symbol representing the model type.
  Description
   Maps the provided model symbol to its corresponding string representation.

  Returns
   the string representation of the specified model.

  """
  def model(model) do
    case model do
      :codestral -> "Codestral-2501"
      :gpt_4o -> "gpt-4o"
      :gpt_4o_mini -> "gpt-4o-mini"
    end
  end

  @spec check_parameters?(params :: keyword()) :: boolean()
  def check_parameters?(params) do
    valid_params = [:max_tokens, :top_p, :temperature]
    Enum.map(params, fn {key, _value} -> key in valid_params end) |> Enum.all?
  end
end
